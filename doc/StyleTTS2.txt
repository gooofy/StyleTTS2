StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models

Yinghao Aaron Li Cong Han Vinay S. Raghavan
Gavin Mischler Nima Mesgarani
Columbia University

Abstract
In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages
style diffusion and adversarial training with large speech language models (SLMs)
to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by
modeling styles as a latent random variable through diffusion models to generate
the most suitable style for the text without requiring reference speech, achieving
efficient latent diffusion while benefiting from the diverse speech synthesis offered
by diffusion models. Furthermore, we employ large pre-trained SLMs, such as
WavLM, as discriminators with our novel differentiable duration modeling for endto-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses
human recordings on the single-speaker LJSpeech dataset and matches it on the
multispeaker VCTK dataset as judged by native English speakers. Moreover, when
trained on the LibriTTS dataset, our model outperforms previous publicly available
models for zero-shot speaker adaptation. This work achieves the first human-level
TTS on both single and multispeaker datasets, showcasing the potential of style
diffusion and adversarial training with large SLMs. The audio demos and source
code are available at https://styletts2.github.io/.

Introduction

Text-to-speech (TTS) synthesis has seen significant advancements in recent years, with numerous
applications such as virtual assistants, audiobooks, and voice-over narration benefiting from increasingly natural and expressive synthetic speech [1, 2]. Some previous works have made significant
progress towards human-level performance [3, 4, 5]. However, the quest for robust and accessible
human-level TTS synthesis remains an ongoing challenge because there is still room for improvement
in terms of diverse and expressive speech [5, 6], robustness for out-of-distribution (OOD) texts [7],
and the requirements of massive datasets for high-performing zero-shot TTS systems [8].
In this paper, we introduce StyleTTS 2, an innovative TTS model that builds upon the style-based
generative model StyleTTS [6] to present the next step towards human-level TTS systems. We model
speech styles as a latent random variable and sample them with a probabilistic diffusion model,
allowing the model to efficiently synthesize highly realistic speech without the need for reference
audio. Since it only needs to sample a style vector instead of the entire speech as a latent variable,
StyleTTS 2 is faster than other diffusion TTS models while still benefiting from the diverse speech
synthesis enabled by diffusion models. One of the key contributions of StyleTTS 2 is the use of
large pre-trained speech language models (SLMs) like Wav2Vec 2.0 [9], HuBERT [10], and WavLM
[11] as discriminators, in conjunction with a novel differentiable duration modeling approach. This
end-to-end (E2E) training setup leverages SLM representations to enhance the naturalness of the
synthesized speech, transferring knowledge from large SLMs for speech generation tasks.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

Our evaluations suggest that speech generated by StyleTTS 2 surpasses human recordings as judged
by native English speakers on the benchmark LJSpeech [12] dataset with statistically significant
comparative mean opinion scores (CMOS) of +0.28 (p < 0.05). Additionally, StyleTTS 2 advances
the state-of-the-art by achieving a CMOS of +1.07 (p ≪ 0.01) compared to NaturalSpeech [5].
Furthermore, it attains human-level performance on the multispeaker VCTK dataset [13] in terms of
naturalness (CMOS = −0.02, p ≫ 0.05) and similarity (CMOS = +0.30, p < 0.1) to the reference
speaker. When trained on a large number of speakers like the LibriTTS dataset [14], StyleTTS 2
demonstrates potential for speaker adaptation. It surpasses previous publicly available models in
this task and outperforms Vall-E [8] in naturalness. Moreover, it achieves slightly worse similarity
to the target speaker with only a 3-second reference speech, despite using around 250 times less
data compared to Vall-E, making it a data-efficient alternative for large pre-training in the zero-shot
speaker adaptation task. As the first model to achieve human-level performance on publicly available
single and multispeaker datasets, StyleTTS 2 sets a new benchmark for TTS synthesis, highlighting
the potential of style diffusion and adversarial training with SLMs for human-level TTS synthesis.

Related Work

Diffusion Models for Speech Synthesis. Diffusion models have gained traction in speech synthesis
due to their potential for diverse speech sampling and fine-grained speech control [15]. They have
been applied to mel-based text-to-speech [16, 17, 18, 19, 20], mel-to-waveform vocoder [21, 22,
23, 24, 25, 26], and end-to-end speech generation [27, 28, 29]. However, their efficiency is limited
compared to non-iterative methods, like GAN-based models [30, 31, 32], due to the need to iteratively
sample mel-spectrograms, waveforms, or other latent representations proportional to the target
speech duration [15]. Furthermore, recent works suggest that state-of-the-art GAN-based models
still perform better than diffusion models in speech synthesis [26, 33]. To address these limitations,
we introduce style diffusion, where a fixed-length style vector is sampled by a diffusion model
conditioned on the input text. This approach significantly improves model speed and enables endto-end training. Notably, StyleTTS 2 synthesizes speech using GAN-based models, with only the
style vector dictating the diversity of speech sampled. This unique combination allows StyleTTS 2 to
achieve high-quality synthesis with fast inference speed while maintaining the benefits of diverse
speech generation, further advancing the capabilities of diffusion models in speech synthesis.
Text-to-Speech with Large Speech Language Models. Recent advancements have proven the
effectiveness of large-scale self-supervised speech language models (SLMs) in enhancing text-tospeech (TTS) quality [34, 35, 36, 37] and speaker adaptation [8, 38, 29, 39]. These works typically
convert text input into either continuous or quantized representations derived from pre-trained SLMs
for speech reconstruction. However, SLM features are not directly optimized for speech synthesis,
while tuning SLMs as a neural codec [34, 35, 8, 29] involves two-stage training. In contrast, our
model benefits from the knowledge of large SLMs via adversarial training using SLM features without
latent space mapping, thus directly learning a latent space optimized for speech synthesis like other
end-to-end (E2E) TTS models. This innovative approach signifies a new direction in TTS with SLMs.
Human-Level Text-to-Speech. Several recent works have advanced towards human-level TTS
[3, 4, 5] using techniques like BERT pre-training [4, 40, 7] and E2E training [32, 5] with differentiable
duration modeling [41, 42]. VITS [3] demonstrates MOS comparable to human recordings on the
LJSpeech and VCTK datasets, while PnG-BERT [4] obtains human-level results on a proprietary
dataset. NaturalSpeech [5], in particular, achieves both MOS and CMOS on LJSpeech statistically
indistinguishable from human recordings. However, we find that there is still room for improvement
in speech quality beyond these state-of-the-art models, as we attain higher performance and set a new
standard for human-level TTS synthesis. Furthermore, recent work shows the necessity for disclosing
the details of evaluation procedures for TTS research [43]. Our evaluation procedures are detailed in
Appendix E, which can be used for reproducible future research toward human-level TTS.

Methods

StyleTTS Overview

StyleTTS [6] is a non-autoregressive TTS framework using a style encoder to derive a style vector
from reference audio, enabling natural and expressive speech generation. The style vector is incorporated into the decoder and duration and prosody predictors using adaptive instance normalization
(AdaIN) [44], allowing the model to generate speech with varying duration, prosody, and emotions.
The model comprises eight modules, organized into three categories: (1) a speech generation system
(acoustic modules) with a text encoder, style encoder, and speech decoder; (2) a TTS prediction
system with duration and prosody predictors; and (3) a utility system for training, including a
discriminator, text aligner, and pitch extractor. It undergoes a two-stage training process: the first
stage trains the acoustic modules for mel-spectrogram reconstruction, and the second trains TTS
prediction modules using the fixed acoustic modules trained in the first stage.
In the first stage, the text encoder T encodes input phonemes t into phoneme representations
htext = T (t), while the text aligner A extracts speech-phoneme alignment aalgn = A(x, t) from input
speech x and phonemes t to produce aligned phoneme representations halgn = htext · aalgn via dot
product. Concurrently, the style encoder E obtains the style vector s = E(x), and the pitch extractor
F extracts the pitch curve px = F (x) along with its energy nx = ∥x∥. Lastly, the speech decoder G
reconstructs x̂ = G (halgn , s, px , nx ), which is trained to match input x using a L1 reconstruction
loss Lmel and adversarial objectives Ladv , Lfm with a discriminator D. Transferable monotonic aligner
(TMA) objectives are also applied to learn optimal alignments (see Appendix G for details).
In the second stage, all components except the discriminator D are fixed, with only the duration
and prosody predictors being trained. The duration predictor S predicts the phoneme duration
with dpred = S(htext , s), whereas the prosody predictor P predicts pitch and energy as p̂x , n̂x =
P (htext , s). The predicted duration is trained to match the ground truth duration dgt derived from
the summed monotonic version of the alignment aalgn along the time axis with an L1 loss Ldur .
The predicted pitch p̂x and energy n̂x are trained to match the ground truth pitch px and energy
nx derived from pitch extractor F with L1 loss Lf 0 and Ln . During inference, dpred is used to
upsample htext through apred , the predicted alignment, obtained by repeating the value 1 for dpred [i]
times at ℓi−1 , where ℓi is the end position of the ith phoneme ti calculated by summing dpred [k] for
k ∈ {1, . . . , i}, and dpred [i] are the predicted duration of ti . The mel-spectrogram is synthesized by
xpred = G(htext · apred , E(x̃), p̂x̃ , n̂x̃ ) with x̃ an arbitrary reference audio that influences the style of
xpred , which is then converted into a waveform using a pre-trained vocoder.
Despite its state-of-the-art performance in synthesizing diverse and controllable speech, StyleTTS
has several drawbacks, such as a two-stage training process with an additional vocoding stage that
degrades sample quality, limited expressiveness due to deterministic generation, and reliance on
reference speech hindering real-time applications.

StyleTTS 2

StyleTTS 2 improves upon the StyleTTS framework, resulting in a more expressive text-to-speech
(TTS) synthesis model with human-level quality and improved out-of-distribution performance.
We introduce an end-to-end (E2E) training process that jointly optimizes all components, along
with direct waveform synthesis and adversarial training with large speech language models (SLMs)
enabled by our innovative differentiable duration modeling. The speech style is modeled as a latent
variable sampled through diffusion models, allowing diverse speech generation without reference
audio. We outline these important changes in the following sections with an overview in Figure 1.
3.2.1

End-to-End Training

E2E training optimizes all TTS system components for inference without relying on any fixed
components like pre-trained vocoders that convert mel-spectrograms into waveforms [3, 32]. To
achieve this, we modify the decoder G to directly generate the waveform from the style vector,
aligned phoneme representations, and pitch and energy curves. We remove the last projection layer
for mel-spectrograms of the decoder and append a waveform decoder after it. We propose two types
of decoders: HifiGAN-based and iSTFTNet-based. The first is based on Hifi-GAN [30], which
directly generates the waveform. In contrast, the iSTFTNet-based decoder [45] produces magnitude
and phase, which are converted into waveforms using inverse short-time Fourier transform for faster
training and inference. We employ the snake activation function [46], proven effective for waveform
generation in [31]. An AdaIN module [44] is added after each activation function to model the style
dependence of the speech, similar to the original StyleTTS decoder. We replace the mel-discriminator
in [6] with the multi-period discriminator (MPD) [30] and multi-resolution discriminator (MRD)

Acoustic modules pre-training and joint training. To accelerate training, the pre-training first optimize
modules inside the blue box; the joint training then follows to optimize all components except the pitch extractor,
which is used to provide the ground truth label for pitch curves. The duration predictor is trained with only Ldur .

(b) SLM adversarial training and inference. WavLM is pre-trained and not tuned. Unlike (a),
the duration predictor is trained E2E with all components using Lslm (eq. 5) via differentiable
upsampling. This process is separate from (a) during training as the input texts can be different, but
the gradients are accumulated for both processes in each batch to update the parameters.

Figure 1: Training and inference scheme of StyleTTS 2 for the single-speaker case. For the multispeaker case, the acoustic and prosodic style encoders (denoted as E) first take reference audio xref
of the target speaker and produce a reference style vector c = E(xref ). The style diffusion model
then takes c as a reference to sample sp and sa that correspond to the speaker in xref .
[47] along with the LSGAN loss functions [48] for decoder training, and incorporate the truncated
pointwise relativistic loss function [49] to enhance sound quality (see Appendix F and G for details).
We found that well-trained acoustic modules, especially the style encoder, can accelerate the training
process for TTS prediction modules. Therefore, before jointly optimizing all components, we first
pre-train the acoustic modules along with the pitch extractor and text aligner via Lmel , Ladv , Lfm and
TMA objectives for N epochs where N depends on the size of the training set, in the same way as
the first training stage of [6]. However, we note that this pre-training is not an absolute necessity:
despite being slower, starting joint training directly from scratch also leads to model convergence,
After acoustic module pre-training, we jointly optimize Lmel , Ladv , Lfm , Ldur , Lf0 and Ln , where Lmel
is modified to match the mel-spectrograms of waveforms reconstructed from predicted pitch p̂x and
energy n̂x (Fig 1a). During joint training, stability issues emerge from diverging gradients, as the
style encoder must encode both acoustic and prosodic information. To address this inconsistency, we
introduce a prosodic style encoder Ep alongside the original acoustic style encoder Ea , previously
denoted as E in section 3.1. Instead of using sa = Ea (x), predictors S and P take sp = Ep (x) as
the input style vector. The style diffusion model generates the augmented style vector s = [sp , sa ].
This modification effectively improves sample quality (see section 5.3). To further decouple the
acoustic modules and predictors, we replace the phoneme representations htext from T , now referred
to as acoustic text encoder, with hbert from another text encoder B based on BERT transformers,
denoted as prosodic text encoder. Specifically, we employ a phoneme-level BERT [7] pre-trained on
extensive corpora of Wikipedia articles as the prosodic text encoder. This approach has been shown
to enhance the naturalness of StyleTTS in the second stage [7], similar to our proposed usage here.
4

With differentiable upsampling and fast style diffusion, we can generate speech samples during
training in a fully differentiable manner, just as during inference. These samples are used to optimize
Lslm (eq. 5) during joint training to update the parameters of all components for inference (Fig 1b).
3.2.2

Style Diffusion

In StyleTTS 2, we model the speech x as a conditional distribution p(x|t) = p(x|t, s)p(s|t) ds
through a latent variable s that follows the distribution p(s|t). We call this variable the generalized
speech style, representing any characteristic in speech beyond phonetic content t, including but not
limited to prosody, lexical stress, formant transitions, and speaking rate [6]. We sample s by EDM
[50] that follows the combined probability flow [51] and time-varying Langevin dynamics [52]:
where σ(τ ) is the noise level schedule and σ̇(τ ) is its time derivative, β(τ ) is the stochasticity term,
W̃τ is the backward Wiener process for τ ∈ [T, 0] and ∇s log pτ (s|t) is the score function at time τ .
We follow the EDM [50] formulation with the denoiser K(s; t, σ) preconditioned as:
Unlike [50] that uses 2nd-order Heun, we solve eq. 4 with the ancestral DPM-2 solver [54] for fast
and diverse sampling as we demand speed more than accuracy. On the other hand, we use the same
scheduler as in [50] with σmin = 0.0001, σmax = 3 and ρ = 9. This combination allows us to sample
a style vector for high-quality speech synthesis with only three steps, equivalent to running a 9-layer
transformer model, minimally impacting the inference speed (see Appendix B for more discussions).
V conditions on t through hbert concatenated with the noisy input E(x) + σξ, and σ is conditioned
via sinusoidal positional embeddings [53]. In the multispeaker setting, we model p(s|t, c) by
K(s; t, c, σ) with an additional speaker embedding c = E(xref ) where xref is the reference audio of
the target speaker. The speaker embedding c is injected into V by adaptive layer normalization [6].
3.2.3

SLM Discriminators

Speech language models (SLMs) encode valuable information ranging from acoustic to semantic
aspects [55], and SLM representations are shown to mimic human perception for evaluating synthesized speech quality [45]. We uniquely transfer knowledge from SLM encoders to generative tasks
via adversarial training by employing a 12-layer WavLM [11] pre-trained on 94k hours of data 1 as
the discriminator. As the number of parameters of WavLM is greater than StyleTTS 2, to avoid discriminator overpowering, we fix the pre-trained WavLM model W and append a convolutional neural
network (CNN) C as the discriminative head. We denote the SLM discriminator DSLM = C ◦ W .
The input audios are downsampled to 16 kHz before being fed into DSLM to match that of WavLM.
C pools features hSLM = W (x) from all layers with a linear map from 13 × 768 to 256 channels.
We train the generator components (T, B, G, S, P, V , denoted as G) and DSLM to optimize:
Lslm = min max (Ex [log DSLM (x)] + Et [log (1 − DSLM (G(t)))]) ,
where DSLM
(x) is the optimal discriminator, T and G represent true and generated data distributions,
while PT and PG are their respective densities. The optimal G∗ is achieved if PW ◦T = PW ◦G , meaning that when converged, G∗ matches the generated and true distributions in the SLM representation
space, effectively mimicking human perception to achieve human-like speech synthesis.

In equation 5, the generator loss is independent of ground truth x and relies only on input text t. This
enables training on out-of-distribution (OOD) texts, which we show in section 5.3 can improve the
performance on OOD texts. In practice, to prevent DSLM from over-fitting on the content of the
speech, we sample in-distribution and OOD texts with equal probability during training.
3.2.4

Differentiable Duration Modeling

The duration predictor produces phoneme durations dpred , but the upsampling method described in
section 3.1 to obtain apred is not differentiable, blocking gradient flow for E2E training. NaturalSpeech
[5] employs an attention-based upsampler [42] for human-level TTS. However, we find this approach
unstable during adversarial training because we train our model using differentiable upsampling
with only the adversarial objective described in eq. 5 and without extra loss terms due to the length
mismatch caused by deviations of dpred from dgt . Although this mismatch can be mitigated with soft
dynamic time warping as used in [42, 5], we find this approach both computationally expensive and
unstable with mel-reconstruction and adversarial objectives. To achieve human-level performance
with adversarial training, a non-parametric upsampling method is preferred for stable training.
Gaussian upsampling [41] is non-parametric and converts the predicted duration dpred into apred [n, i]
using a Gaussian kernel Nci (n; σ) centered at ci := ℓi − 12 dpred [i] with the hyperparameter σ:
where ℓi is the end position and ci is the center position of the ith phoneme ti . However, Gaussian
upsampling has limitations due to its fixed width of Gaussian kernels determined by σ. This constraint
prevents it from accurately modeling alignments with varying lengths depending on dpred . Nonattentive Tacotron [57] extends this by making σi trainable, but the trained parameters introduce
instability for E2E training with adversarial loss, similar to issues of attention-based upsamplers.
We propose a new non-parametric differentiable upsampler without additional training while taking
into account the varying length of the alignment. For each phoneme ti , we model the alignment as a
random variable ai ∈ N, indicating the index of the speech frame the phoneme ti is aligned with. We
define the duration of the ith phoneme as another random variable di ∈ {1, . . . , L}, where L = 50
is the maximum phoneme duration hyperparameter, equivalent to 1.25 seconds in our setting. We
observe that ai =
dk , but each dk is dependent on each other, making the sum difficult to model.
Instead, we approximate ai = di + ℓi−1 . The approximated probability mass function (PMF) of ai is

Experiments

Model Training

We performed experiments on three datasets: LJSpeech, VCTK, and LibriTTS. Our single-speaker
model was trained on the LJSpeech dataset, consisting of 13,100 short audio clips totaling roughly
24 hours. This dataset was divided into training (12,500 samples), validation (100 samples), and
testing (500 samples) sets, with the same split as [3, 5, 6]. The multispeaker model was trained on
VCTK, comprising nearly 44,000 short clips from 109 native speakers with various accents. The data
split was the same as [3], with 43,470 samples for training, 100 for validation, and 500 for testing.
Lastly, we trained our model on the combined LibriTTS train-clean-460 subset [14] for zero-shot
adaptation. This dataset contains about 245 hours of audio from 1,151 speakers. Utterances longer
than 30 seconds or shorter than one second were excluded. We distributed this dataset into training
(98%), validation (1%), and testing (1%) sets, in line with [6]. The test-clean subset was used for
zero-shot adaptation evaluation with 3-second reference clips. All datasets were resampled to 24 kHz
to match LibriTTS, and the texts were converted into phonemes using phonemizer [58].
We used texts in the training split of LibriTTS as the out-of-distribution (OOD) texts for SLM
adversarial training. We used iSTFTNet decoder for LJSpeech due to its speed and sufficient
performance on this dataset, while the HifiGAN decoder was used for the VCTK and LibriTTS
models. Acoustic modules were pre-trained for 100, 50, and 30 epochs on the LJSpeech, VCTK, and
LibriTTS datasets, and joint training followed for 60, 40, and 25 epochs, respectively. We employed
the AdamW optimizer [59] with β1 = 0, β2 = 0.99, weight decay λ = 10−4 , learning rate γ = 10−4
and a batch size of 16 samples for both pre-training and joint training. The loss weights were adopted
from [6] to balance all loss terms (see Appendix G for details). Waveforms were randomly segmented
with a max length of 3 seconds. For SLM adversarial training, both the ground truth and generated
samples were ensured to be 3 to 6 seconds in duration, the same as in fine-tuning of WavLM models
for various downstream tasks [11]. Style diffusion steps were randomly sampled from 3 to 5 during
training for speed and set to 5 during inference for quality. The training was conducted on four
NVIDIA A40 GPUs.

Evaluations

We employed two metrics in our experiments: Mean Opinion Score of Naturalness (MOS-N) for
human-likeness, and Mean Opinion Score of Similarity (MOS-S) for similarity to the reference
for multi-speaker models. These evaluations were conducted by native English speakers from the
U.S. on Amazon Mechanical Turk. All evaluators reported normal hearing and provided informed
consent as monitored by the local institutional review board and in accordance with the ethical
standards of the Declaration of Helsinki 2 . In each test, 80 random text samples from the test set
were selected and converted into speech using our model and the baseline models, along with ground
truth for comparison. Because [7] reported that many TTS models perform poorly for OOD texts, our
LJSpeech experiments also included 40 utterances from Librivox spoken by the narrator of LJSpeech
but from audiobooks not in the original dataset as the ground truth for OOD texts. To compare
the difference between in-distribution and OOD performance, we asked the same raters to evaluate
samples on both in-distribution and OOD texts.
Our baseline models consisted of the three highest-performing public models: VITS [3], StyleTTS [6],
and JETS [32] for LJSpeech; and VITS, YourTTS [60], and StyleTTS for LibriTTS. Each synthesized

We obtained approval for our protocol (number IRB-AAAR8655) from the Institutional Review Board.

Table 1: Comparative mean opinion scores of naturalness and similarity for StyleTTS 2 with p-values
from Wilcoxon test relative to other models. Positive scores indicate StyleTTS 2 is better.
Model

speech set was rated by 5 to 10 evaluators on a 1-5 scale, with increments of 0.5. We randomized
the model order and kept their labels hidden, similar to the MUSHRA approach [61, 62]. We also
conducted Comparative MOS (CMOS) tests to determine statistical significance, as raters can ignore
subtle differences in MOS experiments [3, 7, 5]. Raters were asked to listen to two samples and rate
whether the second was better or worse than the first on a -6 to 6 scale with increments of 1. We
compared our model to the ground truth and NaturalSpeech [5] for LJSpeech, and the ground truth
and VITS for VCTK. For the zero-shot experiment, we compared our LibriTTS model to Vall-E [8].
All baseline models, except for the publicly available VITS model on LibriTTS from ESPNet Toolkit
[63], were official checkpoints released by the authors, including vocoders used in StyleTTS. As
NaturalSpeech and Vall-E are not publicly available, we obtained samples from the authors and the
official Vall-E demo page, respectively. For fairness, we resampled all audio to 22.5 kHz for LJSpeech
and VCTK, and 16 kHz for LibriTTS, to match the baseline models. We conducted ablation studies
using CMOS-N on LJSpeech on OOD texts from LibriTTS test-clean subset for more pronounced
results as in [7]. For more details of our evaluation procedures, please see Appendix E.

Results

5.1

Model Performance

The results outlined in Table 1 establish that StyleTTS 2 outperforms NaturalSpeech by achieving
a CMOS of +1.07 (p ≪ 0.01), setting a new standard for this dataset. Interestingly, StyleTTS 2
was favored over the ground truth with a CMOS of +0.28 (p < 0.05). This preference may stem
from dataset artifacts such as fragmented audiobook passages in the LJSpeech dataset that disrupt
narrative continuity, thus rendering the ground truth narration seemingly unnatural. This hypothesis is
corroborated by the performance of StyleTTS 2 on the VCTK dataset, which lacks narrative context,
where it performs comparably to the ground truth (CMOS = −0.02, p ≫ 0.05). Samples from our
model were more similar to the reference audio speaker than the human recording, suggesting our
model’s effective use of the reference audio for style diffusion. Moreover, StyleTTS 2 scored higher
than the previous state-of-the-art model VITS on VCTK, as evidenced by CMOS-N and CMOS-S.
Consistent with the CMOS results, our model achieved a MOS of 3.83, surpassing all previous
models on LJSpeech (Table 2). In addition, all models, except ours, exhibited some degrees of
quality degradation for out-of-distribution (OOD) texts. This corroborates the gap reported in [7],
with our results providing additional ground truth references. On the other hand, our model did not
show any degradation and significantly outperformed other models in MOS for OOD texts (Table 2),
demonstrating its strong generalization ability and robustness towards OOD texts.
In zero-shot tests, StyleTTS 2 surpasses Table 2: Comparison of MOS with 95% confidence inVall-E in naturalness with a CMOS tervals (CI) on LJSpeech. MOSID represents MOS-N for
of +0.67 (p ≪ 0.01), although it falls in-distribution texts, while MOSOOD is that for OOD texts.
slightly short in similarity (Table 1). Im- Model

a) LJSpeech model.

(b) Unseen speakers on LibriTTS. (c) Zoomed-in unseen speaker.

Figure 2: t-SNE visualization of style vectors sampled via style diffusion from texts in five emotions,
showing that emotions are properly separated for seen and unseen speakers. (a) Clusters of emotion
from styles sampled by the LJSpeech model. (b) Distinct clusters of styles sampled from 5 unseen
speakers by the LibriTTS model. (c) Loose clusters of emotions from Speaker 1 in (b).
MOS-S between StyleTTS and StyleTTS 2 was not statistically significant, hinting at future directions
for improvement in speaker similarity.
5.2

Style Diffusion

Figure 2 shows t-SNE visual- Table 3: Comparison of MOS with 95% confidence intervals (CI)
izations of style vectors created on test-clean subset of LibriTTS for zero-shot speaker adaptation.
using our style diffusion proModel
MOS-N (CI)
MOS-S (CI)
cess. Due to the lack of emotionGround Truth
4.60 (± 0.09) 4.35 (± 0.10)
labeled audiobook text datasets,
StyleTTS 2
4.15 (± 0.11) 4.03 (± 0.11)
we used GPT-4 to generate 500
YourTTS
2.35 (± 0.07) 2.42 (± 0.09)
utterances across five emotions
VITS
3.69 (± 0.12) 3.54 (± 0.13)
for this task [64]. In Figure 2a,
StyleTTS + HiFi-GAN 3.91 (± 0.11) 4.01 (± 0.10)
style vectors from the LJSpeech
model illustrate distinct emotional styles in response to input text sentiment, demonstrating the model’s capability to synthesize
expressive speech in varied emotions without explicit emotion labels during training. This process
was repeated with the LibriTTS model on five unseen speakers, each from a 3-second reference audio.
As depicted in Figure 2b, distinct clusters form for each speaker, showcasing a wide stylistic diversity
derived from a single reference audio. Figure 2c provides a more nuanced view of the first speaker,
revealing visible emotion-based clusters despite some overlaps, indicating that we can manipulate the
emotional tone of an unseen speaker regardless of the tones in the reference audio. These overlaps,
however, can partly explain why the LibriTTS model does not perform as well as the LJSpeech
model, as it is harder to disentangle texts from speakers in the zero-shot setting (see Appendix A.2 for
more results). Table 4 displays our synthesized speech diversity against several baseline models with
the coefficient of variation of duration (CVdur ) and pitch curve (CVf0 ) from the same input text [3].
Our model yields the highest variation, indicating superior potential for generating diverse speech.
Despite being diffusion-based, our model is faster than VITS, FastDiff [28], and ProDiff [18], two of
the fastest diffusion-based TTS models, with 5 iterations of diffusion and iSTFTNet decoder.

Ablation Study


Table 5 details the ablation study, underlying the importance of our proposed components. When
style vectors from style diffusion are substituted with randomly encoded ones as in [6], the CMOS is
−0.46, highlighting the contribution of text-dependent style diffusion to achieving human-level TTS.
Training without our differentiable upsampler and without the SLM discriminator results in a CMOS
of −0.21 and −0.32, validating their key roles in natural speech synthesis. Removing the prosodic
style encoder also yields a −0.35 CMOS. Last, excluding OOD texts from adversarial training leads
to a CMOS of −0.15, proving its efficacy for improving OOD speech synthesis. Table 6 in Appendix
A.3 shows similar outcomes with objective evaluations, further affirming the effectiveness of various
components we proposed in this work. Figure 7 in Appendix D details a layer-wise analysis of input
weights of the SLM discriminator, providing a different view of the efficacy of SLM discriminators.

Conclusions and Limitations

In this study, we present StyleTTS 2, a novel text-to-speech (TTS) model with human-level performance via style diffusion and speech language model discriminators. In particular, it exceeds the
ground truth on LJSpeech and performs on par with it on the VCTK dataset. StyleTTS 2 also shows
potential for zero-shot speaker adaption, with remarkable performance even on limited training data
compared to large-scale models like Vall-E. With our innovative style diffusion method, StyleTTS 2
generates expressive and diverse speech of superior quality while ensuring fast inference time. While
StyleTTS 2 excels in several areas, our results indicate room for improvement in handling large-scale
datasets such as LibriTTS, which contain thousands of or more speakers, acoustic environments,
accents, and other various aspects of speaking styles. The speaker similarity in the aforementioned
zero-shot adaptation speaker task could also benefit from further improvements.
However, zero-shot speaker adaptation has the potential for misuse and deception by mimicking the
voices of individuals as a potential source of misinformation or disinformation. This could lead to
harmful, deceptive interactions such as theft, fraud, harassment, or impersonations of public figures
that may influence political processes or trust in institutions. In order to manage the potential for
harm, we will require users of our model to adhere to a code of conduct that will be clearly displayed
as conditions for using the publicly available code and models. In particular, we will require users to
inform those listening to samples synthesized by StyleTTS 2 that they are listening to synthesized
speech or to obtain informed consent regarding the use of samples synthesized by StyleTTS 2 in
experiments. Users will also be required to use reference speakers who have given consent to have
their voice adapted, either directly or by license. Finally, we will make the source code publicly
available for further research in speaker fraud and impersonation detection.
In addition, while human evaluators have favored StyleTTS 2 over ground truth with statistical
significance on the LJSpeech dataset, this preference may be context-dependent. Original audio
segments from larger contexts like audiobooks could inherently differ in naturalness when isolated,
potentially skewing the evaluations in favor of synthesized speech. Additionally, the inherent
variability in human speech, which is context-independent, might lead to lower ratings when compared
to the more uniform output from StyleTTS 2. Future research should aim to improve evaluation
methods to address these limitations and develop more natural and human-like speech synthesis
models with longer context dependencies.

Acknowledgments

We thank Menoua Keshishian, Vishal Choudhari, and Xilin Jiang for helpful discussions and feedback
on the paper. We also thank Grace Wilsey, Elden Griggs, Jacob Edwards, Rebecca Saez, Moises
Rivera, Rawan Zayter, and D.M. for assessing the quality of synthesized samples and providing
feedback on the quality of models during the development stage of this work. This work was funded
by the national institute of health (NIHNIDCD) and a grant from Marie-Josee and Henry R. Kravis.

