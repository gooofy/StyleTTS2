PHONEME-LEVEL BERT FOR ENHANCED PROSODY OF TEXT-TO-SPEECH WITH GRAPHEME PREDICTIONS
Yinghao Aaron Li, Cong Han, Xilin Jiang, Nima Mesgarani

20 Jan 2023

Department of Electrical Engineering, Columbia University, USA
ABSTRACT
Large-scale pre-trained language models have been shown to
be helpful in improving the naturalness of text-to-speech (TTS)
models by enabling them to produce more naturalistic prosodic
patterns. However, these models are usually word-level or
sup-phoneme-level and jointly trained with phonemes, making them inefficient for the downstream TTS task where only
phonemes are needed. In this work, we propose a phonemelevel BERT (PL-BERT) with a pretext task of predicting
the corresponding graphemes along with the regular masked
phoneme predictions. Subjective evaluations show that our
phoneme-level BERT encoder has significantly improved the
mean opinion scores (MOS) of rated naturalness of synthesized
speech compared with the state-of-the-art (SOTA) StyleTTS
baseline on out-of-distribution (OOD) texts.
Index Terms— Text-to-Speech, Pre-training, BERT,
Transfer learning
1. INTRODUCTION
Text-to-speech (TTS) has seen significant progress in recent
years, and the most recent works are shown to synthesize
speech indistinguishable from natural human speech for indistribution texts evaluated subjectively by human raters [1].
Despite many recent advancements, it remains a challenge
to synthesize natural and expressive speech due to the rich
information contained in the prosody and emotions of human
speech [2]. One crucial aspect that is difficult to capture in
many TTS models is the tone, or the prosody, of speech [3].
Training a TTS model is like learning a language from scratch.
It is crucial to have hundreds of hours of input to learn the
correct intonations and emotions of a foreign language. Even
with these many hours of input, non-native speakers can still
be easily recognized from their intonations and prosodies. TTS
datasets, on the other hand, usually contain far less data than
hundreds of hours due to the requirement of data annotation.
With merely a few hours of data, it is expected that the trained
models will have difficulties capturing naturalistic prosodic
patterns with plain phonemes as input. Hence, large-scale pretrained models are needed to alleviate this problem. BERT, in
particular, has proven effective in improving the performance
of TTS models at either word level[4], character level [5, 6], or sentence level [7].
Despite its success in improving the prosody and naturalness of speech synthesis, these BERT models are not trained
at the phoneme level, even though the input to the downstream TTS task consists of the phonemes only. PnG-BERT
[8] has attempted to tackle this problem by jointly training
with phoneme and grapheme tokens as input and predicting
masked tokens for both phonemes and graphemes. This approach learns richer representations from both graphemes
and phonemes, but it only works for a fixed set of tokens
of graphemes and can fail for unseen words during training. In
addition, the number of tokens for graphemes is prohibitively
large, making the model slow for training and inference. A
recent work, Mixed-Phoneme BERT (MP-BERT) [9], leverages the need for graphemes by training a BERT model that
only takes phonemes as the input. Since the phonemes are
not as linguistically expressive as the graphemes, MP-BERT
also learns a set of sup-phoneme units using the byte-pair
encoding (BPE) [10] that enhances the semantic content of
learned representations. MP-BERT demonstrates performance
comparable to PnG-BERT for downstream TTS tasks, albeit
no grapheme input is required. However, there is no guarantee
that the sup-phoneme units learned through BPE carry as much
linguistic information as graphemes. In addition, the number
of tokens needed for sup-phoneme units is as large as 30,000
in [9], greatly limiting the speed of training and inference.
Here, we propose a phoneme-level BERT model for text-tospeech synthesis. By combining whole-word masked phoneme
and grapheme predictions, we obtain a phoneme-level language model that is more efficient than MP-BERT without
needing graphemes or sup-phoneme units as input. Our contribution lies in the additional pretext task that predicts the
corresponding graphemes for each phoneme (phoneme-tographeme, P2G). By learning a language model directly at the
phoneme level, the model produces representations with a deep
grasp of the dynamics between phonemes, words, and semantics, therefore improving the performance of downstream TTS
tasks. Subjective evaluations show that our phoneme-level
BERT significantly outperforms the current state-of-the-art
baseline StyleTTS model [3] in terms of perceived naturalness of speech for out-of-distribution (OOD) texts. We also
demonstrate that evaluations for in-distribution texts are not
as effective as OOD texts and propose a future direction for

Fig. 1. Pre-training scheme for phoneme-level BERT. M indicates that the input token has been masked. The transformer
encoder takes the phoneme tokens and their position encoding as the input and is trained to predict the masked phoneme tokens
and their corresponding grapheme tokens.
TTS research that shrinks the gap in performance between indistribution and OOD texts. The audio samples can be listened
to at https://pl-bert.github.io/.

2. PHONEME-LEVEL BERT
2.1. Phoneme Representation
In phoneme-level BERT, we only take phonemes as the input because phonemes are the only information needed for
intelligible speech synthesis. We do not use grapheme or supphoneme representations as those used in [8] and [9] because
of the enormous vocabulary size that slows down both training and inference. In addition, extra representations beyond
phonemes suffer from out-of-vocabulary problems where unseen words or sup-phoneme units can occur during inference.
Using phoneme-only representation solves these problems,
making the pre-trained encoder an immediate drop-in replacement for text encoders of any TTS system.
depending on the context in many languages, such as Japanese.
For simplicity, we only use the whole words as tokens to avoid
additional grapheme-phoneme alignment.
2.2.1. Training Objectives
There are two objectives for pre-training: masked phoneme
token prediction (MLM) and phoneme-to-grapheme (P2G)
prediction. As in the original BERT model, we predict the
masked input phoneme tokens from the hidden states of the last
layer using a linear projection along with a softmax function.
The loss function is the cross-entropy loss commonly used for
multi-class prediction. For each phoneme token, we also map
its hidden state to predict its corresponding grapheme with
the same procedure. We calculate the MLM loss values only
for the masked tokens, while we calculate the loss of P2G for
all input tokens. As we show in section 4.2, this objective
is important to learn a meaningful phoneme-level language
representation for significant improvement in TTS tasks. The
training objectives can be written as follows:

2.2. Pre-training
Similar to the original BERT, phoneme-level BERT can be
trained in a self-supervised manner on any corpus where
phonemes and graphemes can be obtained in pairs. The
phonemes and their corresponding graphemes can be prepared using an external grapheme-to-phoneme (G2P) tool.
The graphemes can range from characters to sub-word units to
whole words. Further grapheme-phoneme alignments through
a dynamic programming algorithm may be required because
pronunciations of a character or sub-word unit can change
grapheme token labels, I is the masked indices, E is our
phoneme-level BERT encoder, PM LM is the linear projection
for the MLM task, PP 2G is the linear projection for the P2G
task, N is the total length of the text, and CE(·) denotes the
cross-entropy loss function.
2.2.2. Masking Strategy
Since our goal is to learn a phoneme-level language model,
we need to mask at the word level for the model to learn
meaningful semantic representations. This masking strategy
is termed whole-word masking [11] and is shown to be the
most effective masking strategy for BERT models that take
phonemes as input [8, 9]. We employ the whole word masking
and follow previous works [8, 9, 12] where the phoneme tokens
of 15% of graphemes in each sequence are selected to be
masked at random. When a grapheme is selected, its phonemes
tokens are replaced with a MSK token 80% of the time, are
replaced with random phonemes token 10% of the time, and
stay unchanged 10% of the time.
3. EXPERIMENTS
3.1. Datasets
3.1.1. Text Pre-training Data
We pre-train our phoneme-level BERT model on the English
Wikipedia corpus consisting of 6,280,802 articles and approximately 74M sentences. We divide the dataset into a split
where 6M articles are used for training, 140k articles are used
for validation, and the rest are used for testing. The texts are
normalized to match the pronunciations for each word using
NeMo [13]. Phonemes are obtained using Phonemizer [14]
that converts text sequences into the International Phonetic
Alphabets (IPA) with the eSpeak backend.
3.1.2. TTS Fine-tuning Data
We use the LJSpeech dataset [15] to evaluate the performance
of the downstream TTS tasks. The LJSpeech dataset consists
of 13,100 short audio clips with a total duration of approximately 24 hours. The dataset is divided into a split where the
training set contains 12,500 samples, validation set 100 samples and test set 500 samples. We extract mel-spectrograms
with a FFT size of 2048, hop size of 300, and window length
of 1200 in 80 mel bins. We synthesized waveforms from
mel-spectrograms using Hifi-GAN vocoder [16].
3.2. Training Details
Our phoneme-level BERT is a 12-layer ALBERT [17] model
with a hidden size of 768, an intermediate size of 2,048, and
12 attention heads. The training was conducted on 3 Nvidia
A40 GPUs with a maximum length of 512 tokens and a batch
size of 192 samples. For the MP-BERT baseline, we used the
BPE base dictionary of 30,000 sup-phoneme units as in [9].
The models were trained for 1M steps, roughly 10 epochs.
We fine-tuned our PL-BERT at the second stage of training
of StyleTTS for 100 epochs. We froze the weights of PLBERT for the first 50 epochs and fine-tuned it for another 50
epochs to make the training more stable.
3.3. Evaluations
We performed subjective evaluations on the mean opinion
score of naturalness (MOS) to measure the naturalness of
synthesized speech. We recruited native English speakers
located in the U.S. to participate in the evaluations on Amazon Mechanical Turk. In every experiment, we randomly
selected 30 sentences from the test set of both LJSpeech
dataset (in-distribution) and Gutenberg books dataset [18] (outof-distribution). The latter is considered out-of-distribution
(OOD) because the books used for testing have never been
seen during training for both the pre-trained BERT model and
the fine-tuned TTS model. On the contrary, since the LJSpeech
dataset consists of seven audiobooks, the books to which the
texts in the test set belong are already seen during training.
For each text, we synthesized speech using StyleTTS finetuned with our phoneme-level BERT model, StyleTTS finetuned with MB-BERT, and the baseline StyleTTS model without BERT. The reference audios were selected from the training set with the highest sentence embedding similarity computed using sentence-BERT [19] between the training texts
and the target text. Each speech set was rated by ten raters on a
scale from 1 to 5 with 0.5-point increments. When evaluating
each set, we randomly permuted the order of the models and
instructed the subjects to listen and rate them without telling
them the model labels [20, 21]. We included distorted speech
as the attention checker and all ratings were dropped from our
analysis if the distorted speech was not rated the lowest. In
addition to StyleTTS, we have also included Tacorton 2 [22],
FastSpeech 2 [23], and VITS [24] as baseline models for comparison among OOD texts. To check whether our PL-BERT
model is helpful and its performance is better than MP-BERT,
we also conducted several comparative MOS (CMOS) studies
where the raters were asked to listen to only two samples and
rate whether the second one was better or worse than the first
one. The orders of the samples were shuffled, and the scores
were set on a scale from -6 to 6 with an increment of 1 point.
We further conducted an ablation study to verify the effectiveness of each component in our model. We instructed the
subjects to compare our proposed model to the models with
one component ablated. The ablation study was conducted on
OOD texts for more pronounced results. In addition, we train
a logistic regression P2G predictor on the Wikipedia corpus to
predict graphemes from phonemes to test whether the learned
representation contains contextual grapheme information.
4. RESULTS
4.1. TTS Performance
As shown in Table 1, there is no significant improvement with
PL-BERT on in-distribution texts. However, we can see that
our model has significantly outperformed the baseline model
where no pre-trained BERT is used in Table 2 in terms of both
MOS and CMOS for the out-of-distribution (OOD) texts. In
particular, our model is significantly better than MP-BERT
(Wilcoxon test, p < 0.05), with a CMOS of plus 0.16. This
shows that training with phoneme predictions instead of supphoneme units makes the TTS model generalize better for
unseen texts. We also notice a massive MOS gap between indistribution texts and OOD texts. The MOS difference between
StyleTTS w/ PL-BERT and ground truth is not statistically significant (p > 0.05), although CMOS shows a slight preference
of the raters for the ground truth over our model. However, the
performance drops dramatically when the input texts are OOD.
This performance gap is not specific to StyleTTS models; it is a
prevalent problem for many TTS models, as shown in Table 2.
These MOS scores are significantly worse than those reported
in [3] by roughly 0.4 to 0.5 points. The results suggest that
future works should give more weight to evaluations on OOD
texts. In addition, since our model does not need to process
the sub-phoneme tokens, our model is 1.05 times faster than
MX-BERT on a single NVIDIA A40 GPU.
4.2. Ablation Study
Table 3 shows a slight performance decrease when LP 2G is
removed during training. However, the CMOS is still higher
than the baseline StyleTTS model without BERT. This shows
that using a pre-trained phoneme-level BERT improves downstream TTS tasks even when trained without LP 2G . This
can be attributed to our masking strategy where the entire
grapheme phonemes are masked, so LM LM alone can learn
rich enough linguistic representation that helps the downstream
TTS tasks. However, when LM LM is removed, the CMOS
drops dramatically, indicating that with only LP 2G the trained
model cannot retain the input phonemes information and, therefore, cannot be used for downstream TTS tasks. We note
that the P2G prediction accuracy decreases dramatically when
LP 2G is removed from the training objectives. This shows
that LM LM , even with whole-word masking, does not guarantee that the model learns word-level linguistic representations.
This partly explains why our model performs better than MPBERT, as MP-BERT lacks the LP 2G objective that enables the
model to learn linguistic representations at the phoneme level.

5. CONCLUSIONS
In this work, we proposed phoneme-level BERT, a phonemelevel language model that produces contextualized embeddings
that improve the naturalness and prosody of downstream TTS
tasks. Unlike previous works, our model takes only phonemes
as input, greatly reducing the resources needed during training and inference. We show that our model has significantly
outperformed the baseline StyleTTS model, where no BERT
encoder is fine-tuned with the TTS model, and we also show
that our pre-training strategy is better than MP-BERT for outof-distribution (OOD) texts. We have identified a performance
gap in many existing TTS models between in-distribution and
OOD texts. Since in-distribution texts are barely used for realworld applications, we advocate that future studies focus more
on TTS development for OOD texts.

6. ACKNOWLEDGEMENTS
We thank Gavin Mischler for providing feedback to the quality
of models during the development stage of this work. This
work was funded by the national institute of health (NIHNIDCD) and a grant from Marie-Josee and Henry R. Kravis.

